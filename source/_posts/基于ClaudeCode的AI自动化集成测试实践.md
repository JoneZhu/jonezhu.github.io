---
title: 基于 Claude Code 的 AI 自动化集成测试实践
date: 2025-10-07 03:57:12
categories:
  - 技术
  - 新认知
---
在过去的10个月里，AI 辅助编程工具已经从"代码生成器"进化为真正的"开发伙伴"。当我还沉浸在 AI 能够生成高质量代码时，一个新的变化正在形成：**AI 不仅可以写代码，还可以自动化完成集成测试和调试**。

这篇文章将分享我在投篮人分组功能开发中的实践经验，展示如何让 Claude Code 完成从代码实现到集成测试的全流程自动化。

### 传统开发流程的时间分配

在 AI 辅助编程时代，开发流程可以概括为三个阶段：

```
准备上下文 → Code Review/评估 → 集成测试/调试
   (40%)         (20%)              (40%)
```

随着 AI 代码生成能力的成熟，**代码编写效率已经得到了极大提升**。但我们很快发现，一个新的瓶颈出现了：**集成测试和调试仍然占据了 50% 的开发时间**。

### 为什么集成测试如此耗时？

这背后的根本原因是：**程序不是构想出来的，是验证出来的**。

物理世界的复杂性决定了：
- 程序的行为只有在真实运行时才能被准确评估
- 系统集成层面的问题需要通过实际测试才能发现

传统的调试流程是一个往复循环：

```
启动服务 → 调用接口 → 查看日志 → 检查数据库 → 修改代码 → 重启服务 → ...
```

每次迭代都需要人工操作，耗时且容易出错。**如果 AI 能够自动完成这个闭环，开发效率将得到质的飞跃。**

## 解决方案：让 AI 完成集成测试闭环

### 核心思路

既然人类调试的过程是"启动→测试→观察→修改→重启"的循环，那么让 AI 工具具备同样的能力即可：

1. **启动服务的能力** - 准备启动脚本
2. **观察运行状态的能力** - 连接数据库、查看日志
3. **执行测试的能力** - 准备测试数据、调用接口
4. **分析和修复的能力** - 读取日志、修改代码、重新测试

### 技术准备

为了让 Claude Code 具备完整的集成测试能力，需要准备以下环境：

#### 1. 启动脚本（start.sh）

```bash
#!/bin/bash
# 简单的启动脚本，让 AI 可以快速启动服务
./gradlew bootRun > logs/app.log 2>&1 &
```

**作用**：让 AI 可以一键启动后端服务，无需理解复杂的构建过程。

#### 2. 数据库访问（MCP MySQL Server）

使用 [mysql-mcp-server](https://github.com/dpflucas/mysql-mcp-server) 让 AI 能够直接查询数据库：

```json
{
  "mcpServers": {
    "mysql": {
      "command": "npx",
      "args": ["-y", "@dpflucas/mysql-mcp-server"],
      "env": {
        "MYSQL_HOST": "127.0.0.1",
        "MYSQL_PORT": "3306",
        "MYSQL_USER": "xxxxx",
        "MYSQL_PASSWORD": "xxxxx",
        "MYSQL_DATABASE": "xxxxx"
      }
    }
  }
}
```

**作用**：AI 可以随时查询数据库状态，验证数据正确性。

#### 3. 测试数据

准备标准测试数据集：
```json
// docs/投篮人分组300张图片支持测试数据.json
{
  "video_id_1": ["image_base64_1", "image_base64_2", ...],
  "video_id_2": ["image_base64_1", "image_base64_2", ...],
  ...
}
```

**作用**：提供可复现的测试场景，便于验证功能正确性。

#### 4. 简化的认证机制

```java
// JwtAuthenticationFilter.java
if ("87654321".equals(jwt)) {
    // 测试环境使用简单 token
    UserDetails userDetails = User.builder()
        .username("xxxxxxx")
        .password("")
        .authorities(Collections.emptyList())
        .build();
}
```

**作用**：降低测试复杂度，让 AI 可以轻松调用受保护的接口。

## 实践案例：投篮人分组功能的完整开发

### 需求背景

开发一个支持 300 张图片的投篮人分组功能，原有系统限制为 50 张。需要实现：
- 多批次处理（每批 50 张）
- AI 智能聚合（跨批次人员识别）
- 完整的错误处理和降级策略

### AI 自动化开发过程

#### 阶段 1：需求分析与方案设计（AI 辅助）

我只需要提供需求：
> "GroupingProcessingJob 目前支持最多 50 张图片，现在要支持 300 张。思路是每 50 个一组分批处理，最后聚合结果。你梳理一下代码，思考如何聚合。"

Claude Code 自动：
1. 读取相关代码文件
2. 查询数据库了解数据结构
3. 分析 AI 请求日志理解交互模式
4. 生成详细技术方案文档

**耗时**：5 分钟（传统需要 30-60 分钟）

#### 阶段 2：代码实现（AI 完成）

基于方案文档，AI 自动实现：
- `AiProviderStrategyV2` 接口设计
- `OpenAIStrategyV2` 和 `VolcEngineStrategyV2` 实现
- `GroupingAiServiceImplV2` 多批次处理逻辑
- 聚合提示词构建（400+ 行复杂逻辑）
- 完整的错误处理和日志记录

**代码量**：~1500 行新增/修改
**耗时**：15 分钟（传统需要 4-6 小时）

#### 阶段 3：集成测试与调试（AI 自动化）

这是最关键的部分。传统开发中，这个阶段需要：
1. 手动启动服务
2. 准备测试请求
3. 发送请求并等待响应
4. 查看日志定位问题
5. 查询数据库验证数据
6. 修改代码
7. 重启服务重新测试

**传统耗时**：2-4 小时

而 Claude Code 的自动化测试过程：

**第一轮测试**：
```bash
# AI 自动执行
./start.sh                          # 启动服务
sleep 60                            # 等待启动完成
curl -X POST .../grouping/submit    # 提交测试任务
# 发现错误：totalVideos 字段缺失
```

AI 查看错误日志，自动修复测试脚本，添加缺失字段。

**第二轮测试**：
```bash
# AI 重新测试
curl -X POST ... -d '{"totalVideos": 136, ...}'
# 发现错误：用户不存在 
```

AI 查询数据库：
```sql
SELECT phone FROM users LIMIT 10;
-- 发现实际用户
```

AI 自动修改 `JwtAuthenticationFilter.java`，替换测试用户。

**第三轮测试**：
```bash
# AI 重启服务并测试
pkill -f 'like-sports-api'
./start.sh
curl -X POST ...
# 发现错误：批次大小超出 API 限制（100 > 50）
```

AI 修改 `application.yml`：
```yaml
max-batch-size: 50  # 从 100 改为 50
```

**第四轮测试**：
```bash
# AI 再次测试
# 发现错误：merge 阶段 API 返回 HTML 而非 JSON
```

AI 分析代码，发现 API 端点错误：
```java
// 错误：缺少 /v1/ 前缀
baseUrl + "/chat/completions"
// 修复
baseUrl + "/v1/chat/completions"
```

**第五轮测试**：
```bash
# AI 最终测试
# ✅ 所有批次成功
# ✅ 聚合阶段成功
# ✅ 最终结果正确
```

AI 验证数据库：
```sql
SELECT task_id, batch_number, response_status, response_time_ms
FROM ai_request_logs WHERE task_id = 22;

-- 批次1: SUCCESS, 30.3s
-- 批次2: SUCCESS, 24.2s
-- 批次3: SUCCESS, 18.9s
-- 聚合:  SUCCESS, 23.4s ✅
```

**自动化测试耗时**：30 分钟（包括 5 轮调试）
**人工干预**：0 次
**Bug 修复**：4 个（全部由 AI 自动完成）

### 测试结果

最终测试数据：

| 指标 | 结果 |
|------|------|
| 输入视频数 | 136 个 |
| 处理批次数 | 3 批 + 1 聚合 |
| 总处理时间 | 97 秒 |
| 成功率 | 100% (4/4) |
| 分组视频数 | 131/136 (96.3%) |
| 最终人数 | 16 人 |

## 核心价值分析

### 效率提升对比

| 阶段 | 传统开发 | AI 自动化 | 提升比例 |
|------|----------|-----------|----------|
| 需求分析 | 30-60 分钟 | 5 分钟 | **6-12x** |
| 代码实现 | 4-6 小时 | 15 分钟 | **16-24x** |
| 集成测试 | 2-4 小时 | 30 分钟 | **4-8x** |
| **总计** | **7-11 小时** | **50 分钟** | **8-13x** |

### 质量提升

1. **更全面的测试覆盖**
   - AI 会检查数据库状态
   - AI 会验证日志输出
   - AI 会测试边界条件

2. **更快的问题定位**
   - AI 可以同时查看代码、日志、数据库
   - AI 具备上下文记忆，能关联多个线索

3. **更可靠的修复**
   - AI 会在修复后重新测试
   - AI 会验证修复是否引入新问题

### 开发体验改善

**传统开发**：
```
写代码 → 启动服务 ⏰ → 测试 → 看日志 → 查数据库 → 定位问题 →
改代码 → 重启服务 ⏰ → 再测试 → ... (循环往复，疲惫不堪)
```

**AI 自动化**：
```
提需求 → ☕️ 喝杯咖啡 → ✅ 功能完成
```

开发者可以专注于：
- 业务逻辑设计
- 架构决策
- Code Review
- 产品优化

而不是陷入无穷无尽的调试循环。

## 关键技术要点

### 1. 环境标准化

**原则**：让 AI 可以轻松启动和操作你的开发环境。

```bash
# ✅ 好的实践：简单的启动脚本
./start.sh

# ❌ 坏的实践：复杂的启动过程
export ENV=dev
export DB_HOST=...
./gradlew clean build
java -jar -Dspring.profiles.active=dev ...
```

### 2. 可观测性

**原则**：让 AI 能够"看到"系统运行状态。

必备能力：
- ✅ 日志文件访问（`logs/app.log`）
- ✅ 数据库查询（MCP MySQL Server）
- ✅ API 调用能力（curl/http）
- ✅ 进程管理（启动/停止/重启）

### 3. 测试数据管理

**原则**：提供可复现的测试场景。

```json
// 好的测试数据
{
  "scenario": "multi_batch_grouping",
  "description": "测试 136 个视频的多批次分组",
  "expected_batches": 3,
  "data": { ... }
}
```

### 4. 简化的认证机制

**原则**：降低测试复杂度，但不影响生产环境安全。

```java
// 测试环境特殊处理
if (isTestEnvironment && "7654321".equals(token)) {
    return testUser;
}
```

### 目前的挑战

❌ **移动端功能**
- 需要真机/模拟器
- 涉及复杂的用户交互
- 难以自动化观察状态

❌ **复杂的 UI 交互**
- 需要视觉验证
- 交互路径复杂
- 状态不透明

### 改进方向

**对于移动端功能的探索**：

当前我们的项目是"重手机端、轻服务端"的架构，想让 AI 直接体验手机功能仍然面临技术挑战：

1. **模拟器集成**：让 AI 能控制 Android/iOS 模拟器
2. **UI 自动化**：类似 Appium 的工具集成
3. **视觉验证**：AI 能够"看"屏幕并判断 UI 是否正确
4. **状态管理**：将手机端状态暴露给 AI

这些方向值得持续探索。

## 总结与展望
我们正在见证一个新的开发范式的诞生：

**Vibe Coding 2.0 = AI 代码生成 + AI 自动化测试**

在这个范式下：
- 开发者专注于"What"（做什么）
- AI 负责"How"（怎么做）和"Verify"（是否正确）

### 对行业的启示

1. **重新思考开发流程**
   - 不要把 AI 当作"代码补全工具"
   - 而是当作"全流程开发伙伴"

2. **投资基础设施**
   - 构建 AI 友好的开发环境
   - 标准化工具链和测试流程


**软件开发的本质正在改变**：从"人类写代码、人类测试"，到"AI 写代码、AI 测试、人类验证"。
